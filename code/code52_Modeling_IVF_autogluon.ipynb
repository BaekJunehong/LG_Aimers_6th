{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 난임 환자 대상 임신 성공 여부 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGAimers 6th 온라인 해커톤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "IVF_train = pd.read_csv('../data/IVF_train_dataset_52.csv')\n",
    "IVF_test = pd.read_csv('../data/IVF_test_dataset_52.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import autogluon.core as ag\n",
    "\n",
    "train_data = TabularDataset(IVF_train)\n",
    "test_data = TabularDataset(IVF_test)\n",
    "\n",
    "label = '임신_성공_여부'\n",
    "eval_metric = 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.8\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          16\n",
      "Memory Avail:       8.78 GB / 15.86 GB (55.3%)\n",
      "Disk Space Avail:   178.53 GB / 476.30 GB (37.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-02-24 21:24:14,021\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_2\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Beginning AutoGluon training ... Time limit = 142s\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m AutoGluon will save models to \"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_2\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Train Data Rows:    222268\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Train Data Columns: 85\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Label Column:       임신_성공_여부\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tAvailable Memory:                    8010.42 MB\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tTrain Data (Original)  Memory Usage: 155.59 MB (1.9% of available memory)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t\tNote: Converting 49 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tUnused Original Features (Count: 4): ['ID', '불임_원인_-_정자_면역학적_요인', '불임_원인_-_정자_운동성', '배란_유도_유형_기록되지않은시행']\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t('bool', [])   : 1 | ['배란_유도_유형_기록되지않은시행']\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t('int', [])    : 2 | ['불임_원인_-_정자_면역학적_요인', '불임_원인_-_정자_운동성']\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t('object', []) : 1 | ['ID']\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t('float', []) : 33 | ['시술_당시_나이', '임신_시도_또는_마지막_임신_경과_연수', '단일_배아_이식_여부', '착상_전_유전_검사_사용_여부', '착상_전_유전_진단_사용_여부', ...]\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t('int', [])   : 48 | ['배란_자극_여부', '남성_주_불임_원인', '남성_부_불임_원인', '여성_주_불임_원인', '여성_부_불임_원인', ...]\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t('float', [])     : 23 | ['시술_당시_나이', '임신_시도_또는_마지막_임신_경과_연수', '총_생성_배아_수', '미세주입된_난자_수', '미세주입에서_생성된_배아_수', ...]\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t('int', [])       : 12 | ['총_시술_횟수', '클리닉_내_총_시술_횟수', 'IVF_시술_횟수', 'DI_시술_횟수', '총_임신_횟수', ...]\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t\t('int', ['bool']) : 46 | ['배란_자극_여부', '단일_배아_이식_여부', '착상_전_유전_검사_사용_여부', '착상_전_유전_진단_사용_여부', '남성_주_불임_원인', ...]\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t2.2s = Fit runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t81 features in original data used to generate 81 features in processed data.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tTrain Data (Processed) Memory Usage: 69.10 MB (0.9% of available memory)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Data preprocessing and feature engineering runtime = 2.42s ...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 93.31s of the 139.99s of remaining time.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.6491\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.26s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t33.82s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 55.71s of the 102.39s of remaining time.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.6339\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.26s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t38.82s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 16.35s of the 63.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.12%)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.7374\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t7.6s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t1.66s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 5.08s of the 51.76s of remaining time.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.43%)\n",
      "\u001b[36m(_ray_fit pid=10076)\u001b[0m \tRan out of time, early stopping on iteration 125. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=10076)\u001b[0m \t[124]\tvalid_set's binary_logloss: 0.491362\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.7375\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t4.08s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t1.04s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 140.00s of the 43.90s of remaining time.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 0.542, 'LightGBMXT_BAG_L1': 0.458}\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.7381\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t3.07s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 40.77s of the 40.74s of remaining time.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.61%)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.7376\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t5.21s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.83s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 32.07s of the 32.04s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=15356)\u001b[0m \tRan out of time, early stopping on iteration 113. Best iteration is:\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=15356)\u001b[0m \t[113]\tvalid_set's binary_logloss: 0.491947\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.74%)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.7377\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t5.5s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.7s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 22.85s of the 22.82s of remaining time.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 127 due to low time. Expected time usage reduced from 52.7s -> 22.5s...\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.7437\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t16.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t5.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 140.00s of the 1.20s of remaining time.\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \tEnsemble Weights: {'RandomForestGini_BAG_L2': 0.667, 'LightGBMXT_BAG_L1': 0.167, 'LightGBM_BAG_L1': 0.167}\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.7448\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t5.13s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m AutoGluon training complete, total runtime = 146.46s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2437.9 rows/s (44454 batch size)\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_2\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=22448)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L2       0.736774   0.738098     roc_auc        0.801179       2.722608  14.752702                 0.017004                0.029007           3.066193            2       True          5\n",
      "1          LightGBM_BAG_L2       0.736617   0.737669     roc_auc       10.444338      76.031186  17.713871                 0.293065                0.697163           5.501755            2       True          7\n",
      "2          LightGBM_BAG_L1       0.736411   0.737494     roc_auc        0.275061       1.036230   4.083400                 0.275061                1.036230           4.083400            1       True          4\n",
      "3        LightGBMXT_BAG_L1       0.736193   0.737375     roc_auc        0.509115       1.657371   7.603109                 0.509115                1.657371           7.603109            1       True          3\n",
      "4        LightGBMXT_BAG_L2       0.736188   0.737580     roc_auc       10.471344      76.159209  17.424282                 0.320072                0.825185           5.212166            2       True          6\n",
      "5      WeightedEnsemble_L3       0.731989   0.744825     roc_auc       10.518355      80.396164  33.363684                 0.012002                0.030007           5.131346            3       True          9\n",
      "6  RandomForestGini_BAG_L2       0.727204   0.743650     roc_auc       10.506353      80.366157  28.232339                 0.355080                5.032133          16.020223            2       True          8\n",
      "7    KNeighborsUnif_BAG_L1       0.650722   0.649050     roc_auc        4.656042      33.820729   0.261549                 4.656042               33.820729           0.261549            1       True          1\n",
      "8    KNeighborsDist_BAG_L1       0.633620   0.633948     roc_auc        4.711055      38.819694   0.264058                 4.711055               38.819694           0.264058            1       True          2\n",
      "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
      "\t168s\t = DyStack   runtime |\t432s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=0.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
      "Beginning AutoGluon training ... Time limit = 432s\n",
      "AutoGluon will save models to \"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_2\"\n",
      "Train Data Rows:    250052\n",
      "Train Data Columns: 85\n",
      "Label Column:       임신_성공_여부\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    8031.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 175.04 MB (2.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 49 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 3): ['ID', '불임_원인_-_정자_면역학적_요인', '배란_유도_유형_기록되지않은시행']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])   : 1 | ['배란_유도_유형_기록되지않은시행']\n",
      "\t\t('int', [])    : 1 | ['불임_원인_-_정자_면역학적_요인']\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 33 | ['시술_당시_나이', '임신_시도_또는_마지막_임신_경과_연수', '단일_배아_이식_여부', '착상_전_유전_검사_사용_여부', '착상_전_유전_진단_사용_여부', ...]\n",
      "\t\t('int', [])   : 49 | ['배란_자극_여부', '남성_주_불임_원인', '남성_부_불임_원인', '여성_주_불임_원인', '여성_부_불임_원인', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 23 | ['시술_당시_나이', '임신_시도_또는_마지막_임신_경과_연수', '총_생성_배아_수', '미세주입된_난자_수', '미세주입에서_생성된_배아_수', ...]\n",
      "\t\t('int', [])       : 12 | ['총_시술_횟수', '클리닉_내_총_시술_횟수', 'IVF_시술_횟수', 'DI_시술_횟수', '총_임신_횟수', ...]\n",
      "\t\t('int', ['bool']) : 47 | ['배란_자극_여부', '단일_배아_이식_여부', '착상_전_유전_검사_사용_여부', '착상_전_유전_진단_사용_여부', '남성_주_불임_원인', ...]\n",
      "\t2.2s = Fit runtime\n",
      "\t82 features in original data used to generate 82 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 77.98 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.35s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 430.03s of the 430.03s of remaining time.\n",
      "\t0.6506\t = Validation score   (roc_auc)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t57.34s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 369.73s of the 369.72s of remaining time.\n",
      "\t0.6352\t = Validation score   (roc_auc)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t57.2s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 311.90s of the 311.90s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.31%)\n",
      "\t0.7373\t = Validation score   (roc_auc)\n",
      "\t9.02s\t = Training   runtime\n",
      "\t2.26s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 298.85s of the 298.84s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.58%)\n",
      "\t0.7377\t = Validation score   (roc_auc)\n",
      "\t7.87s\t = Training   runtime\n",
      "\t1.21s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 288.46s of the 288.45s of remaining time.\n",
      "\t0.7285\t = Validation score   (roc_auc)\n",
      "\t42.33s\t = Training   runtime\n",
      "\t13.7s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 231.60s of the 231.59s of remaining time.\n",
      "\t0.7292\t = Validation score   (roc_auc)\n",
      "\t46.25s\t = Training   runtime\n",
      "\t14.05s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 170.49s of the 170.48s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=7.26%)\n",
      "\t0.7378\t = Validation score   (roc_auc)\n",
      "\t60.59s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 107.54s of the 107.53s of remaining time.\n",
      "\t0.7279\t = Validation score   (roc_auc)\n",
      "\t48.01s\t = Training   runtime\n",
      "\t15.8s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 42.79s of the 42.78s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 198 due to low time. Expected time usage reduced from 64.0s -> 42.4s...\n",
      "\t0.7277\t = Validation score   (roc_auc)\n",
      "\t33.08s\t = Training   runtime\n",
      "\t10.46s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -1.57s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.389, 'LightGBM_BAG_L1': 0.333, 'LightGBMXT_BAG_L1': 0.222, 'RandomForestEntr_BAG_L1': 0.056}\n",
      "\t0.7384\t = Validation score   (roc_auc)\n",
      "\t8.4s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 442.49s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 7839.5 rows/s (50011 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_2\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# 시간 제한 설정 (예: 10분)\n",
    "time_limit =  1 * 10 * 60\n",
    "\n",
    "# # GPU를 사용할 수 없는 모델을 제외하도록 설정\n",
    "# exclude_model_types = [\n",
    "#     'KNN',  # K-Nearest Neighbors\n",
    "#     'RF',   # Random Forest\n",
    "#     'XT',   # Extra Trees\n",
    "#     'LR',   # Linear Regression\n",
    "#     'NN'    # Tabular Neural Network\n",
    "# ]\n",
    "\n",
    "# TabularPredictor 객체 생성 및 학습\n",
    "predictor = TabularPredictor(\n",
    "    label=label,\n",
    "    eval_metric=eval_metric,\n",
    "    path='AutogluonModels/ag-20250224_2'  # 모델 저장 경로\n",
    ").fit(\n",
    "    train_data,\n",
    "    presets='best_quality',  # 'best_quality', 'medium_quality', 'good_quality' 등의 프리셋 설정\n",
    "    # num_stack_levels=0,  # 스택 레벨 설정 / dynamic_stacking=True(디폴트)인 경우 무시\n",
    "    num_bag_folds=5,  # 배깅 설정\n",
    "    time_limit=time_limit,  # 시간 제한 설정\n",
    "    # num_gpus=1,  # GPU 사용 설정\n",
    "    # excluded_model_types=exclude_model_types  # 제외할 모델 유형 설정\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0      WeightedEnsemble_L2   0.738352     roc_auc      17.648948  132.137389   \n",
      "1          CatBoost_BAG_L1   0.737824     roc_auc       0.097027   60.594738   \n",
      "2          LightGBM_BAG_L1   0.737656     roc_auc       1.208277    7.866764   \n",
      "3        LightGBMXT_BAG_L1   0.737325     roc_auc       2.256603    9.019058   \n",
      "4  RandomForestEntr_BAG_L1   0.729227     roc_auc      14.048031   46.254408   \n",
      "5  RandomForestGini_BAG_L1   0.728451     roc_auc      13.699092   42.332958   \n",
      "6    ExtraTreesGini_BAG_L1   0.727913     roc_auc      15.800915   48.009892   \n",
      "7    ExtraTreesEntr_BAG_L1   0.727666     roc_auc      10.460503   33.077937   \n",
      "8    KNeighborsUnif_BAG_L1   0.650569     roc_auc      57.338782    0.322073   \n",
      "9    KNeighborsDist_BAG_L1   0.635238     roc_auc      57.195124    0.326074   \n",
      "\n",
      "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                0.039010           8.402420            2       True   \n",
      "1                0.097027          60.594738            1       True   \n",
      "2                1.208277           7.866764            1       True   \n",
      "3                2.256603           9.019058            1       True   \n",
      "4               14.048031          46.254408            1       True   \n",
      "5               13.699092          42.332958            1       True   \n",
      "6               15.800915          48.009892            1       True   \n",
      "7               10.460503          33.077937            1       True   \n",
      "8               57.338782           0.322073            1       True   \n",
      "9               57.195124           0.326074            1       True   \n",
      "\n",
      "   fit_order  \n",
      "0         10  \n",
      "1          7  \n",
      "2          4  \n",
      "3          3  \n",
      "4          6  \n",
      "5          5  \n",
      "6          8  \n",
      "7          9  \n",
      "8          1  \n",
      "9          2  \n"
     ]
    }
   ],
   "source": [
    "print(predictor.leaderboard(silent = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.feature_importance(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 모델 가져오기\n",
    "model_to_use = predictor.model_best\n",
    "\n",
    "# 확률 예측\n",
    "prob_predictions = predictor.predict_proba(test_data, model=model_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID  probability\n",
      "0  TEST_00000     0.001556\n",
      "1  TEST_00001     0.002249\n",
      "2  TEST_00002     0.151962\n",
      "3  TEST_00003     0.103942\n",
      "4  TEST_00004     0.512530\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과를 test_data에 추가\n",
    "test_data['probability'] = prob_predictions.iloc[:, 1]\n",
    "\n",
    "# 최종 제출 파일 생성\n",
    "submission = test_data[['ID', 'probability']]\n",
    "submission = submission.sort_values(by='ID')\n",
    "\n",
    "# 제출 파일 저장\n",
    "submission.to_csv('../submission/code52_IVF_lgbm.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# 예측 결과 확인\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이콘 PUBLIC xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
