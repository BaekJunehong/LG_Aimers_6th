{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 난임 환자 대상 임신 성공 여부 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGAimers 6th 온라인 해커톤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "IVF_train = pd.read_csv('../data/IVF_train_dataset_53.csv')\n",
    "IVF_test = pd.read_csv('../data/IVF_test_dataset_53.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "import autogluon.core as ag\n",
    "\n",
    "train_data = TabularDataset(IVF_train)\n",
    "test_data = TabularDataset(IVF_test)\n",
    "\n",
    "label = '임신_성공_여부'\n",
    "eval_metric = 'roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.11.8\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "CPU Count:          16\n",
      "Memory Avail:       10.48 GB / 15.86 GB (66.1%)\n",
      "Disk Space Avail:   183.40 GB / 476.30 GB (38.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 150s of the 600s of remaining time (25%).\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
      "2025-02-24 22:06:42,602\tINFO worker.py:1810 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_code53_IVF\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Beginning AutoGluon training ... Time limit = 143s\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m AutoGluon will save models to \"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_code53_IVF\\ds_sub_fit\\sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Train Data Rows:    222268\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Train Data Columns: 95\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Label Column:       임신_성공_여부\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tAvailable Memory:                    9800.96 MB\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tTrain Data (Original)  Memory Usage: 172.54 MB (1.8% of available memory)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t\tNote: Converting 49 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tUnused Original Features (Count: 4): ['ID', '불임_원인_-_정자_면역학적_요인', '불임_원인_-_정자_운동성', '배란_유도_유형_기록되지않은시행']\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t('bool', [])   : 1 | ['배란_유도_유형_기록되지않은시행']\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t('int', [])    : 2 | ['불임_원인_-_정자_면역학적_요인', '불임_원인_-_정자_운동성']\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t('object', []) : 1 | ['ID']\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t('float', []) : 43 | ['시술_당시_나이', '임신_시도_또는_마지막_임신_경과_연수', '단일_배아_이식_여부', '착상_전_유전_검사_사용_여부', '착상_전_유전_진단_사용_여부', ...]\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t('int', [])   : 48 | ['배란_자극_여부', '남성_주_불임_원인', '남성_부_불임_원인', '여성_주_불임_원인', '여성_부_불임_원인', ...]\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t('float', [])     : 33 | ['시술_당시_나이', '임신_시도_또는_마지막_임신_경과_연수', '총_생성_배아_수', '미세주입된_난자_수', '미세주입에서_생성된_배아_수', ...]\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t('int', [])       : 12 | ['총_시술_횟수', '클리닉_내_총_시술_횟수', 'IVF_시술_횟수', 'DI_시술_횟수', '총_임신_횟수', ...]\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t\t('int', ['bool']) : 46 | ['배란_자극_여부', '단일_배아_이식_여부', '착상_전_유전_검사_사용_여부', '착상_전_유전_진단_사용_여부', '남성_주_불임_원인', ...]\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t2.4s = Fit runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t91 features in original data used to generate 91 features in processed data.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tTrain Data (Processed) Memory Usage: 86.06 MB (0.9% of available memory)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Data preprocessing and feature engineering runtime = 2.56s ...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 93.81s of the 140.75s of remaining time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.6495\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.36s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t39.78s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 51.16s of the 98.09s of remaining time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.6342\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.32s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t43.86s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 6.67s of the 53.60s of remaining time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.09%)\n",
      "\u001b[36m(_ray_fit pid=24848)\u001b[0m \tRan out of time, early stopping on iteration 153. Best iteration is:\n",
      "\u001b[36m(_ray_fit pid=24848)\u001b[0m \t[152]\tvalid_set's binary_logloss: 0.49145\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.7378\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t5.45s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t1.4s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 140.75s of the 44.11s of remaining time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.7378\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t2.49s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 41.56s of the 41.53s of remaining time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.38%)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.7377\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t8.76s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t1.25s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 29.10s of the 29.06s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=12960)\u001b[0m \tRan out of time, early stopping on iteration 142. Best iteration is:\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=12960)\u001b[0m \t[142]\tvalid_set's binary_logloss: 0.49005\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.40%)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.7385\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t7.75s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t1.18s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 17.48s of the 17.44s of remaining time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 106 due to low time. Expected time usage reduced from 47.9s -> 17.1s...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 12.11s compared to 10s of available time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 2.91s of the 2.87s of remaining time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tWarning: Model is expected to require 51.5s to train, which exceeds the maximum time limit of 2.6s, skipping model...\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tTime limit exceeded... Skipping RandomForestEntr_BAG_L2.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 1.67s of the 1.63s of remaining time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.73%)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tTime limit exceeded... Skipping CatBoost_BAG_L2.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 140.75s of the -3.32s of remaining time.\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L2': 0.667, 'LightGBMXT_BAG_L1': 0.167, 'LightGBMXT_BAG_L2': 0.167}\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.7387\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t4.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m \t0.03s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m AutoGluon training complete, total runtime = 150.78s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 2161.7 rows/s (44454 batch size)\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_code53_IVF\\ds_sub_fit\\sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=23984)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                   model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0    WeightedEnsemble_L3       0.736460   0.738678     roc_auc       13.797070      87.501022  26.649722                 0.016003                0.030006           4.020904            3       True          7\n",
      "1      LightGBMXT_BAG_L2       0.736372   0.737727     roc_auc       13.419986      86.289755  14.881069                 0.383086                1.252279           8.760577            2       True          5\n",
      "2      LightGBMXT_BAG_L1       0.736307   0.737810     roc_auc        0.487109       1.396315   5.445342                 0.487109                1.396315           5.445342            1       True          3\n",
      "3    WeightedEnsemble_L2       0.736307   0.737810     roc_auc        0.503113       1.427322   7.937062                 0.016004                0.031007           2.491720            2       True          4\n",
      "4        LightGBM_BAG_L2       0.736190   0.738514     roc_auc       13.397981      86.218738  13.868241                 0.361081                1.181261           7.747749            2       True          6\n",
      "5  KNeighborsUnif_BAG_L1       0.652924   0.649468     roc_auc        6.554785      39.778339   0.358078                 6.554785               39.778339           0.358078            1       True          1\n",
      "6  KNeighborsDist_BAG_L1       0.634689   0.634189     roc_auc        5.995007      43.862823   0.317071                 5.995007               43.862823           0.317071            1       True          2\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t174s\t = DyStack   runtime |\t426s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 426s\n",
      "AutoGluon will save models to \"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_code53_IVF\"\n",
      "Train Data Rows:    250052\n",
      "Train Data Columns: 95\n",
      "Label Column:       임신_성공_여부\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9521.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 194.11 MB (2.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 49 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 3): ['ID', '불임_원인_-_정자_면역학적_요인', '배란_유도_유형_기록되지않은시행']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('bool', [])   : 1 | ['배란_유도_유형_기록되지않은시행']\n",
      "\t\t('int', [])    : 1 | ['불임_원인_-_정자_면역학적_요인']\n",
      "\t\t('object', []) : 1 | ['ID']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 43 | ['시술_당시_나이', '임신_시도_또는_마지막_임신_경과_연수', '단일_배아_이식_여부', '착상_전_유전_검사_사용_여부', '착상_전_유전_진단_사용_여부', ...]\n",
      "\t\t('int', [])   : 49 | ['배란_자극_여부', '남성_주_불임_원인', '남성_부_불임_원인', '여성_주_불임_원인', '여성_부_불임_원인', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 33 | ['시술_당시_나이', '임신_시도_또는_마지막_임신_경과_연수', '총_생성_배아_수', '미세주입된_난자_수', '미세주입에서_생성된_배아_수', ...]\n",
      "\t\t('int', [])       : 12 | ['총_시술_횟수', '클리닉_내_총_시술_횟수', 'IVF_시술_횟수', 'DI_시술_횟수', '총_임신_횟수', ...]\n",
      "\t\t('int', ['bool']) : 47 | ['배란_자극_여부', '단일_배아_이식_여부', '착상_전_유전_검사_사용_여부', '착상_전_유전_진단_사용_여부', '남성_주_불임_원인', ...]\n",
      "\t2.3s = Fit runtime\n",
      "\t92 features in original data used to generate 92 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 97.06 MB (1.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 2.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 110 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 282.44s of the 423.75s of remaining time.\n",
      "\t0.6512\t = Validation score   (roc_auc)\n",
      "\t0.41s\t = Training   runtime\n",
      "\t63.33s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 216.00s of the 357.32s of remaining time.\n",
      "\t0.636\t = Validation score   (roc_auc)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t64.5s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 150.75s of the 292.06s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=6.84%)\n",
      "\t0.7381\t = Validation score   (roc_auc)\n",
      "\t9.76s\t = Training   runtime\n",
      "\t1.73s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 137.00s of the 278.31s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=7.02%)\n",
      "\t0.7376\t = Validation score   (roc_auc)\n",
      "\t8.74s\t = Training   runtime\n",
      "\t1.13s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 125.52s of the 266.84s of remaining time.\n",
      "\t0.7272\t = Validation score   (roc_auc)\n",
      "\t45.08s\t = Training   runtime\n",
      "\t15.33s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 64.27s of the 205.58s of remaining time.\n",
      "\t0.7282\t = Validation score   (roc_auc)\n",
      "\t45.79s\t = Training   runtime\n",
      "\t15.49s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2.17s of the 143.49s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=7.47%)\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 140.06s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.611, 'LightGBM_BAG_L1': 0.333, 'RandomForestEntr_BAG_L1': 0.056}\n",
      "\t0.7384\t = Validation score   (roc_auc)\n",
      "\t5.19s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 134.80s of the 134.76s of remaining time.\n",
      "2025-02-24 22:14:24,317\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-02-24 22:14:24,319\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-02-24 22:14:24,321\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-02-24 22:14:24,322\tERROR worker.py:422 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=7.84%)\n",
      "\t0.738\t = Validation score   (roc_auc)\n",
      "\t8.01s\t = Training   runtime\n",
      "\t1.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 124.43s of the 124.38s of remaining time.\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (5 workers, per: cpus=3, gpus=0, memory=7.39%)\n",
      "\t0.7381\t = Validation score   (roc_auc)\n",
      "\t7.83s\t = Training   runtime\n",
      "\t0.89s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 114.07s of the 114.03s of remaining time.\n",
      "\t0.7502\t = Validation score   (roc_auc)\n",
      "\t45.15s\t = Training   runtime\n",
      "\t16.32s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 51.71s of the 51.67s of remaining time.\n",
      "\tWarning: Reducing model 'n_estimators' from 300 -> 235 due to low time. Expected time usage reduced from 65.4s -> 51.3s...\n",
      "\t0.7488\t = Validation score   (roc_auc)\n",
      "\t37.71s\t = Training   runtime\n",
      "\t13.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 0.03s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestGini_BAG_L2': 0.6, 'RandomForestEntr_BAG_L2': 0.4}\n",
      "\t0.7514\t = Validation score   (roc_auc)\n",
      "\t8.84s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 435.24s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 1235.6 rows/s (50011 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"c:\\Users\\juneh\\OneDrive\\바탕 화면\\git_Aimers6th\\LG_Aimers_6th\\code\\AutogluonModels\\ag-20250224_code53_IVF\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# 시간 제한 설정 \n",
    "time_limit =  1 * 10 * 60\n",
    "\n",
    "# # GPU를 사용할 수 없는 모델을 제외하도록 설정\n",
    "# exclude_model_types = [\n",
    "#     'KNN',  # K-Nearest Neighbors\n",
    "#     'RF',   # Random Forest\n",
    "#     'XT',   # Extra Trees\n",
    "#     'LR',   # Linear Regression\n",
    "#     'NN'    # Tabular Neural Network\n",
    "# ]\n",
    "\n",
    "# TabularPredictor 객체 생성 및 학습\n",
    "predictor = TabularPredictor(\n",
    "    label=label,\n",
    "    eval_metric=eval_metric,\n",
    "    path='AutogluonModels/ag-20250224_code53_IVF'  # 모델 저장 경로\n",
    ").fit(\n",
    "    train_data,\n",
    "    presets='best_quality',  # 'best_quality', 'medium_quality', 'good_quality' 등의 프리셋 설정\n",
    "    # num_stack_levels=0,  # 스택 레벨 설정 / dynamic_stacking=True(디폴트)인 경우 무시\n",
    "    num_bag_folds=5,  # 배깅 설정\n",
    "    time_limit=time_limit,  # 시간 제한 설정\n",
    "    # num_gpus=1,  # GPU 사용 설정\n",
    "    # excluded_model_types=exclude_model_types  # 제외할 모델 유형 설정\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      model  score_val eval_metric  pred_time_val    fit_time  \\\n",
      "0       WeightedEnsemble_L3   0.751401     roc_auc     190.925792  201.868731   \n",
      "1   RandomForestGini_BAG_L2   0.750244     roc_auc     177.835828  155.321147   \n",
      "2   RandomForestEntr_BAG_L2   0.748814     roc_auc     174.566615  147.883931   \n",
      "3       WeightedEnsemble_L2   0.738394     roc_auc      18.383380   69.473045   \n",
      "4         LightGBMXT_BAG_L1   0.738130     roc_auc       1.733899    9.755707   \n",
      "5           LightGBM_BAG_L2   0.738057     roc_auc     162.404857  118.008112   \n",
      "6         LightGBMXT_BAG_L2   0.738045     roc_auc     162.536890  118.184644   \n",
      "7           LightGBM_BAG_L1   0.737600     roc_auc       1.128252    8.740956   \n",
      "8   RandomForestEntr_BAG_L1   0.728154     roc_auc      15.488222   45.790831   \n",
      "9   RandomForestGini_BAG_L1   0.727166     roc_auc      15.327763   45.079168   \n",
      "10    KNeighborsUnif_BAG_L1   0.651183     roc_auc      63.333550    0.409091   \n",
      "11    KNeighborsDist_BAG_L1   0.635952     roc_auc      64.500974    0.399091   \n",
      "\n",
      "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
      "0                 0.036008           8.838496            3       True   \n",
      "1                16.323169          45.146304            2       True   \n",
      "2                13.053956          37.709087            2       True   \n",
      "3                 0.033007           5.185551            2       True   \n",
      "4                 1.733899           9.755707            1       True   \n",
      "5                 0.892199           7.833269            2       True   \n",
      "6                 1.024231           8.009801            2       True   \n",
      "7                 1.128252           8.740956            1       True   \n",
      "8                15.488222          45.790831            1       True   \n",
      "9                15.327763          45.079168            1       True   \n",
      "10               63.333550           0.409091            1       True   \n",
      "11               64.500974           0.399091            1       True   \n",
      "\n",
      "    fit_order  \n",
      "0          12  \n",
      "1          10  \n",
      "2          11  \n",
      "3           7  \n",
      "4           3  \n",
      "5           9  \n",
      "6           8  \n",
      "7           4  \n",
      "8           6  \n",
      "9           5  \n",
      "10          1  \n",
      "11          2  \n"
     ]
    }
   ],
   "source": [
    "print(predictor.leaderboard(silent = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.feature_importance(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적의 모델 가져오기\n",
    "model_to_use = predictor.model_best\n",
    "\n",
    "# 확률 예측\n",
    "prob_predictions = predictor.predict_proba(test_data, model=model_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           ID  probability\n",
      "0  TEST_00000     0.001028\n",
      "1  TEST_00001     0.000996\n",
      "2  TEST_00002     0.144262\n",
      "3  TEST_00003     0.083543\n",
      "4  TEST_00004     0.451441\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과를 test_data에 추가\n",
    "test_data['probability'] = prob_predictions.iloc[:, 1]\n",
    "\n",
    "# 최종 제출 파일 생성\n",
    "submission = test_data[['ID', 'probability']]\n",
    "submission = submission.sort_values(by='ID')\n",
    "\n",
    "# 제출 파일 저장\n",
    "submission.to_csv('../submission/code53_IVF_lgbm.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# 예측 결과 확인\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이콘 PUBLIC xx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
