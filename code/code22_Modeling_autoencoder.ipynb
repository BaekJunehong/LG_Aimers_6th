{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 난임 환자 대상 임신 성공 여부 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGAimers 6th 온라인 해커톤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "IVF_train = pd.read_csv('../data/IVF_train_dataset_20.csv')\n",
    "IVF_test = pd.read_csv('../data/IVF_test_dataset_20.csv')\n",
    "\n",
    "DI_train = pd.read_csv('../data/DI_train_dataset_20.csv')\n",
    "DI_test = pd.read_csv('../data/DI_test_dataset_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID 열을 제외한 특성과 타겟 변수 분리\n",
    "IVF_X = IVF_train.drop(['임신_성공_여부', 'ID'], axis=1)\n",
    "IVF_y = IVF_train['임신_성공_여부']\n",
    "\n",
    "DI_X = DI_train.drop(['임신_성공_여부', 'ID'], axis=1)\n",
    "DI_y = DI_train['임신_성공_여부']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "IVF_categorical_columns = [\n",
    "    \"시술_시기_코드\",\n",
    "    \"시술_당시_나이\",\n",
    "    \"임신_시도_또는_마지막_임신_경과_연수\",\n",
    "    \"배란_유도_유형\",\n",
    "    \"배아_생성_주요_이유\",\n",
    "    \"난자_출처\",\n",
    "    \"정자_출처\",\n",
    "    \"난자_기증자_나이\",\n",
    "    \"정자_기증자_나이\",\n",
    "    \"변환된_특정_시술_유형\",\n",
    "    \"채취_해동_차이\",\n",
    "    \"해동_혼합_차이\",\n",
    "    \"혼합_이식_차이\",\n",
    "    \"이식_해동_차이\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_categorical_columns = [\n",
    "    \"시술_시기_코드\",\n",
    "    \"시술_당시_나이\",\n",
    "    \"임신_시도_또는_마지막_임신_경과_연수\",\n",
    "    \"정자_기증자_나이\",\n",
    "    \"변환된_특정_시술_유형\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 범주형 변수를 문자열로 변환\n",
    "IVF_X[IVF_categorical_columns] = IVF_X[IVF_categorical_columns].astype(str)\n",
    "DI_X[DI_categorical_columns] = DI_X[DI_categorical_columns].astype(str)\n",
    "IVF_test[IVF_categorical_columns] = IVF_test[IVF_categorical_columns].astype(str)\n",
    "DI_test[DI_categorical_columns] = DI_test[DI_categorical_columns].astype(str)\n",
    "\n",
    "# OrdinalEncoder를 사용하여 범주형 변수 인코딩\n",
    "IVF_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "DI_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "IVF_X[IVF_categorical_columns] = IVF_encoder.fit_transform(IVF_X[IVF_categorical_columns])\n",
    "DI_X[DI_categorical_columns] = DI_encoder.fit_transform(DI_X[DI_categorical_columns])\n",
    "IVF_test[IVF_categorical_columns] = IVF_encoder.transform(IVF_test[IVF_categorical_columns])\n",
    "DI_test[DI_categorical_columns] = DI_encoder.transform(DI_test[DI_categorical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "IVF_X_train, IVF_X_test, IVF_y_train, IVF_y_test = train_test_split(IVF_X, IVF_y, test_size=0.2, random_state=42)\n",
    "DI_X_train, DI_X_test, DI_y_train, DI_y_test = train_test_split(DI_X, DI_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test 데이터 전부 사용해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일(code22_auto.csv) 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 모델 파라미터 설정\n",
    "IVF_model_params = {\n",
    "    'n_estimators': 4471,\n",
    "    'num_leaves': 13,\n",
    "    'max_depth': 279,\n",
    "    'learning_rate': 0.007075124517450591,\n",
    "    'min_child_samples': 26,\n",
    "    'subsample': 0.29772991936701476,\n",
    "    'colsample_bytree': 0.8913054521763838,\n",
    "    'reg_alpha': 0.0004860363321690653,\n",
    "    'reg_lambda': 311.08056657247363,\n",
    "    'min_split_gain': 0.18214905183450955,\n",
    "    'random_state': 42,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "DI_model_params = {\n",
    "    'n_estimators': 1816,\n",
    "    'num_leaves': 3926,\n",
    "    'max_depth': 259,\n",
    "    'learning_rate': 0.00238377640011148,\n",
    "    'min_child_samples': 1,\n",
    "    'subsample': 0.7610056627240331,\n",
    "    'colsample_bytree': 0.6655579164853634,\n",
    "    'reg_alpha': 0.00025227758337188327,\n",
    "    'reg_lambda': 76.744107215122684,\n",
    "    'min_split_gain': 0.007773520329665474,\n",
    "    'random_state': 42,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# LGBM 모델 학습 및 예측\n",
    "IVF_model_final = lgb.LGBMClassifier(**IVF_model_params)\n",
    "IVF_model_final.fit(IVF_X, IVF_y)\n",
    "IVF_test_pred = IVF_model_final.predict(IVF_test.drop('ID', axis=1))\n",
    "\n",
    "DI_model_final = lgb.LGBMClassifier(**DI_model_params)\n",
    "DI_model_final.fit(DI_X, DI_y)\n",
    "DI_test_pred = DI_model_final.predict(DI_test.drop('ID', axis=1))\n",
    "\n",
    "# K-Fold를 사용하여 IVF와 DI의 잘못된 예측 데이터 수집\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "IVF_wrong_predictions = []\n",
    "DI_wrong_predictions = []\n",
    "\n",
    "# K-Fold 교차 검증을 통한 잘못된 예측 수집 함수\n",
    "def collect_wrong_predictions(X, y, wrong_predictions, model_params):\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model = lgb.LGBMClassifier(**model_params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        wrong_idx = val_idx[y_pred != y_val.values]\n",
    "        wrong_predictions.append(X.iloc[wrong_idx])\n",
    "\n",
    "collect_wrong_predictions(IVF_X, IVF_y, IVF_wrong_predictions, IVF_model_params)\n",
    "collect_wrong_predictions(DI_X, DI_y, DI_wrong_predictions, DI_model_params)\n",
    "\n",
    "# 오토인코더 정의\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# IVF 오토인코더 학습\n",
    "IVF_input_dim = IVF_wrong_predictions[0].shape[1]\n",
    "IVF_autoencoder = Autoencoder(IVF_input_dim)\n",
    "IVF_criterion = nn.MSELoss()\n",
    "IVF_optimizer = optim.Adam(IVF_autoencoder.parameters(), lr=0.01)\n",
    "\n",
    "IVF_data_tensor = torch.tensor(pd.concat(IVF_wrong_predictions).values, dtype=torch.float32)\n",
    "IVF_dataloader = DataLoader(TensorDataset(IVF_data_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(50):\n",
    "    for batch in IVF_dataloader:\n",
    "        IVF_optimizer.zero_grad()\n",
    "        output = IVF_autoencoder(batch[0])\n",
    "        loss = IVF_criterion(output, batch[0])\n",
    "        loss.backward()\n",
    "        IVF_optimizer.step()\n",
    "\n",
    "# DI 오토인코더 학습\n",
    "DI_input_dim = DI_wrong_predictions[0].shape[1]\n",
    "DI_autoencoder = Autoencoder(DI_input_dim)\n",
    "DI_criterion = nn.MSELoss()\n",
    "DI_optimizer = optim.Adam(DI_autoencoder.parameters(), lr=0.01)\n",
    "\n",
    "DI_data_tensor = torch.tensor(pd.concat(DI_wrong_predictions).values, dtype=torch.float32)\n",
    "DI_dataloader = DataLoader(TensorDataset(DI_data_tensor), batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(50):\n",
    "    for batch in DI_dataloader:\n",
    "        DI_optimizer.zero_grad()\n",
    "        output = DI_autoencoder(batch[0])\n",
    "        loss = DI_criterion(output, batch[0])\n",
    "        loss.backward()\n",
    "        DI_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제출 파일(code22_auto.csv) 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "# IVF 테스트 데이터 보정\n",
    "IVF_test_tensor = torch.tensor(IVF_test.drop(['ID'], axis=1).values, dtype=torch.float32)\n",
    "IVF_predicted_output = IVF_autoencoder(IVF_test_tensor).detach().numpy()\n",
    "IVF_reconstruction_error = np.mean((IVF_test.drop(['ID'], axis=1).values - IVF_predicted_output) ** 2, axis=1)\n",
    "IVF_threshold = np.percentile(IVF_reconstruction_error, 95)\n",
    "\n",
    "# DI 테스트 데이터 보정\n",
    "DI_test_tensor = torch.tensor(DI_test.drop(['ID'], axis=1).values, dtype=torch.float32)\n",
    "DI_predicted_output = DI_autoencoder(DI_test_tensor).detach().numpy()\n",
    "DI_reconstruction_error = np.mean((DI_test.drop(['ID'], axis=1).values - DI_predicted_output) ** 2, axis=1)\n",
    "DI_threshold = np.percentile(DI_reconstruction_error, 95)\n",
    "\n",
    "# IVF 예측 수정\n",
    "IVF_test_prob = IVF_model_final.predict_proba(IVF_test.drop('ID', axis=1))[:, 1]\n",
    "IVF_test_prob[IVF_reconstruction_error > IVF_threshold] = np.where(\n",
    "    IVF_test_prob[IVF_reconstruction_error > IVF_threshold] < 0.5,\n",
    "    IVF_test_prob[IVF_reconstruction_error > IVF_threshold] + 0.5,\n",
    "    1 - IVF_test_prob[IVF_reconstruction_error > IVF_threshold]\n",
    ")\n",
    "\n",
    "# DI 예측 수정\n",
    "DI_test_prob = DI_model_final.predict_proba(DI_test.drop('ID', axis=1))[:, 1]\n",
    "DI_test_prob[DI_reconstruction_error > DI_threshold] = np.where(\n",
    "    DI_test_prob[DI_reconstruction_error > DI_threshold] < 0.5,\n",
    "    DI_test_prob[DI_reconstruction_error > DI_threshold] + 0.5,\n",
    "    1 - DI_test_prob[DI_reconstruction_error > DI_threshold]\n",
    ")\n",
    "\n",
    "# 최종 제출 파일 생성\n",
    "IVF_submission = pd.DataFrame({'ID': IVF_test['ID'], 'probability': IVF_test_prob})\n",
    "DI_submission = pd.DataFrame({'ID': DI_test['ID'], 'probability': DI_test_prob})\n",
    "\n",
    "submission = pd.concat([IVF_submission, DI_submission])\n",
    "submission.to_csv('../submission/code22_auto.csv', index=False)\n",
    "\n",
    "print(\"제출 파일(code22_auto.csv) 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4504\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 두 CSV 파일을 읽어옵니다.\n",
    "df1 = pd.read_csv('../submission/code22_auto.csv')\n",
    "df2 = pd.read_csv('../submission/code22_submit.csv')\n",
    "\n",
    "# ID를 기준으로 병합합니다.\n",
    "merged_df = pd.merge(df1, df2, on='ID', suffixes=('_df1', '_df2'))\n",
    "\n",
    "# 'probability' 열을 비교합니다.\n",
    "comparison = merged_df['probability_df1'] == merged_df['probability_df2']\n",
    "\n",
    "# 차이점이 있는 행을 출력합니다.\n",
    "differences = merged_df[~comparison]\n",
    "\n",
    "# differences 데이터셋의 길이를 출력합니다.\n",
    "print(len(differences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>probability_df1</th>\n",
       "      <th>probability_df2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>TEST_00039</td>\n",
       "      <td>0.501018</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>TEST_00052</td>\n",
       "      <td>0.625715</td>\n",
       "      <td>0.125715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>TEST_00135</td>\n",
       "      <td>0.501148</td>\n",
       "      <td>0.001148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>TEST_00184</td>\n",
       "      <td>0.631045</td>\n",
       "      <td>0.131045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>TEST_00206</td>\n",
       "      <td>0.501132</td>\n",
       "      <td>0.001132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>TEST_00237</td>\n",
       "      <td>0.501186</td>\n",
       "      <td>0.001186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>TEST_00260</td>\n",
       "      <td>0.501192</td>\n",
       "      <td>0.001192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>TEST_00270</td>\n",
       "      <td>0.661021</td>\n",
       "      <td>0.161021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>TEST_00288</td>\n",
       "      <td>0.501175</td>\n",
       "      <td>0.001175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>TEST_00293</td>\n",
       "      <td>0.501136</td>\n",
       "      <td>0.001136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID  probability_df1  probability_df2\n",
       "38   TEST_00039         0.501018         0.001018\n",
       "50   TEST_00052         0.625715         0.125715\n",
       "130  TEST_00135         0.501148         0.001148\n",
       "178  TEST_00184         0.631045         0.131045\n",
       "200  TEST_00206         0.501132         0.001132\n",
       "230  TEST_00237         0.501186         0.001186\n",
       "253  TEST_00260         0.501192         0.001192\n",
       "263  TEST_00270         0.661021         0.161021\n",
       "280  TEST_00288         0.501175         0.001175\n",
       "285  TEST_00293         0.501136         0.001136"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "differences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이콘 PUBLIC 0.6957687046"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
