{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 난임 환자 대상 임신 성공 여부 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGAimers 6th 온라인 해커톤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, X_train, y_train, X_test, y_test):\n",
    "    # 모델 학습\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 예측\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # 평가\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print()\n",
    "    print(\"--- Model Performance ---\")\n",
    "    print(f\"Model Accuracy: {accuracy}\")\n",
    "    print(f\"Model F1 Score: {f1}\")\n",
    "    print(f\"Model AUC: {auc}\")\n",
    "    \n",
    "    # 혼동 행렬 출력\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "IVF_train = pd.read_csv('../data/IVF_train_dataset_54.csv')\n",
    "IVF_test = pd.read_csv('../data/IVF_test_dataset_54.csv')\n",
    "\n",
    "DI_train = pd.read_csv('../data/DI_train_dataset_54.csv')\n",
    "DI_test = pd.read_csv('../data/DI_test_dataset_54.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID 열을 제외한 특성과 타겟 변수 분리\n",
    "IVF_X = IVF_train.drop(['임신_성공_여부', 'ID'], axis=1)\n",
    "IVF_y = IVF_train['임신_성공_여부']\n",
    "\n",
    "DI_X = DI_train.drop(['임신_성공_여부', 'ID'], axis=1)\n",
    "DI_y = DI_train['임신_성공_여부']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVF_X shape: (250052, 77)\n",
      "IVF_test shape: (87891, 77)\n",
      "DI_X shape: (6289, 31)\n",
      "DI_test shape: (2176, 31)\n"
     ]
    }
   ],
   "source": [
    "print(f\"IVF_X shape: {IVF_X.shape}\")\n",
    "print(f\"IVF_test shape: {IVF_test.drop('ID', axis=1).shape}\")\n",
    "\n",
    "print(f\"DI_X shape: {DI_X.shape}\")\n",
    "print(f\"DI_test shape: {DI_test.drop('ID', axis=1).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IVF_categorical_columns = [\n",
    "    \"시술_시기_코드\",\n",
    "    \"시술_당시_나이\",\n",
    "    \"특정_시술_유형\",\n",
    "    \"배란_유도_유형\",\n",
    "    \"난자_출처\",\n",
    "    \"정자_출처\",\n",
    "    \"난자_기증자_나이\",\n",
    "    \"정자_기증자_나이\",\n",
    "    \"채취_해동_차이\",\n",
    "    \"해동_혼합_차이\",\n",
    "    \"혼합_이식_차이\",\n",
    "    \"이식_해동_차이\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DI_categorical_columns = [\n",
    "    \"시술_시기_코드\",\n",
    "    \"시술_당시_나이\",\n",
    "    \"특정_시술_유형\",\n",
    "    \"정자_기증자_나이\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 범주형 변수를 문자열로 변환\n",
    "IVF_X[IVF_categorical_columns] = IVF_X[IVF_categorical_columns].astype(str)\n",
    "DI_X[DI_categorical_columns] = DI_X[DI_categorical_columns].astype(str)\n",
    "IVF_test[IVF_categorical_columns] = IVF_test[IVF_categorical_columns].astype(str)\n",
    "DI_test[DI_categorical_columns] = DI_test[DI_categorical_columns].astype(str)\n",
    "\n",
    "# OrdinalEncoder를 사용하여 범주형 변수 인코딩\n",
    "IVF_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "DI_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "IVF_X[IVF_categorical_columns] = IVF_encoder.fit_transform(IVF_X[IVF_categorical_columns])\n",
    "DI_X[DI_categorical_columns] = DI_encoder.fit_transform(DI_X[DI_categorical_columns])\n",
    "IVF_test[IVF_categorical_columns] = IVF_encoder.transform(IVF_test[IVF_categorical_columns])\n",
    "DI_test[DI_categorical_columns] = DI_encoder.transform(DI_test[DI_categorical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "IVF_X_train, IVF_X_test, IVF_y_train, IVF_y_test = train_test_split(IVF_X, IVF_y, test_size=0.2, random_state=42)\n",
    "DI_X_train, DI_X_test, DI_y_train, DI_y_test = train_test_split(DI_X, DI_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVF 최종 예측: [0 0 0 ... 0 0 0]\n",
      "DI 최종 예측: [0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# 전역 모델 변수 선언\n",
    "lgb_model = None\n",
    "xgb_model = None\n",
    "cat_model = None\n",
    "\n",
    "# Base 모델 훈련 및 예측 함수 정의\n",
    "def train_and_predict(X_train, y_train, X_val):\n",
    "    global lgb_model, xgb_model, cat_model  # 전역 변수 사용 선언\n",
    "\n",
    "    # 모델 정의\n",
    "    lgb_model = lgb.LGBMClassifier(verbose=-1)\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    cat_model = CatBoostClassifier(verbose=0)\n",
    "\n",
    "    # 모델 훈련\n",
    "    lgb_model.fit(X_train, y_train)\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    cat_model.fit(X_train, y_train)\n",
    "\n",
    "    # 예측 확률 생성\n",
    "    lgb_pred_proba = lgb_model.predict_proba(X_val)[:, 1]\n",
    "    xgb_pred_proba = xgb_model.predict_proba(X_val)[:, 1]\n",
    "    cat_pred_proba = cat_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    return lgb_pred_proba, xgb_pred_proba, cat_pred_proba\n",
    "\n",
    "# IVF 데이터셋에 대한 훈련 및 예측\n",
    "IVF_X_train, IVF_X_val, IVF_y_train, IVF_y_val = train_test_split(IVF_X, IVF_y, test_size=0.2, random_state=42)\n",
    "\n",
    "lgbm_ivf_proba, xgb_ivf_proba, cat_ivf_proba = train_and_predict(IVF_X_train, IVF_y_train, IVF_X_val)\n",
    "\n",
    "# 메타모델 입력 데이터 생성\n",
    "meta_X_ivf = np.column_stack((lgbm_ivf_proba, xgb_ivf_proba, cat_ivf_proba))\n",
    "\n",
    "# 메타모델 훈련\n",
    "meta_model_ivf = LogisticRegression()\n",
    "meta_model_ivf.fit(meta_X_ivf, IVF_y_val)\n",
    "\n",
    "# IVF 테스트 데이터에 대한 예측\n",
    "lgbm_ivf_test_proba = lgb_model.predict_proba(IVF_test.drop('ID', axis=1))[:, 1]\n",
    "xgb_ivf_test_proba = xgb_model.predict_proba(IVF_test.drop('ID', axis=1))[:, 1]\n",
    "cat_ivf_test_proba = cat_model.predict_proba(IVF_test.drop('ID', axis=1))[:, 1]\n",
    "\n",
    "meta_X_ivf_test = np.column_stack((lgbm_ivf_test_proba, xgb_ivf_test_proba, cat_ivf_test_proba))\n",
    "final_ivf_predictions = meta_model_ivf.predict(meta_X_ivf_test)\n",
    "\n",
    "# DI 데이터셋에 대한 훈련 및 예측\n",
    "DI_X_train, DI_X_val, DI_y_train, DI_y_val = train_test_split(DI_X, DI_y, test_size=0.2, random_state=42)\n",
    "\n",
    "lgbm_di_proba, xgb_di_proba, cat_di_proba = train_and_predict(DI_X_train, DI_y_train, DI_X_val)\n",
    "\n",
    "# 메타모델 입력 데이터 생성\n",
    "meta_X_di = np.column_stack((lgbm_di_proba, xgb_di_proba, cat_di_proba))\n",
    "\n",
    "# 메타모델 훈련\n",
    "meta_model_di = LogisticRegression()\n",
    "meta_model_di.fit(meta_X_di, DI_y_val)\n",
    "\n",
    "# DI 테스트 데이터에 대한 예측\n",
    "lgbm_di_test_proba = lgb_model.predict_proba(DI_test.drop('ID', axis=1))[:, 1]\n",
    "xgb_di_test_proba = xgb_model.predict_proba(DI_test.drop('ID', axis=1))[:, 1]\n",
    "cat_di_test_proba = cat_model.predict_proba(DI_test.drop('ID', axis=1))[:, 1]\n",
    "\n",
    "meta_X_di_test = np.column_stack((lgbm_di_test_proba, xgb_di_test_proba, cat_di_test_proba))\n",
    "final_di_predictions = meta_model_di.predict(meta_X_di_test)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"IVF 최종 예측:\", final_ivf_predictions)\n",
    "print(\"DI 최종 예측:\", final_di_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [87891, 250052]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# 메타모델 훈련\u001b[39;00m\n\u001b[0;32m     51\u001b[0m meta_model_ivf \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[1;32m---> 52\u001b[0m \u001b[43mmeta_model_ivf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_X_ivf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIVF_y\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# IVF 데이터에 대한 메타모델 학습\u001b[39;00m\n\u001b[0;32m     54\u001b[0m meta_model_di \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m     55\u001b[0m meta_model_di\u001b[38;5;241m.\u001b[39mfit(meta_X_di, DI_y)  \u001b[38;5;66;03m# DI 데이터에 대한 메타모델 학습\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1201\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1199\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1201\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1209\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1281\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1264\u001b[0m     X,\n\u001b[0;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1277\u001b[0m )\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1281\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\juneh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [87891, 250052]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 모델 정의\n",
    "lgb_model_ivf = lgb.LGBMClassifier(verbose=-1)\n",
    "xgb_model_ivf = xgb.XGBClassifier()\n",
    "cat_model_ivf = CatBoostClassifier(verbose=0)\n",
    "\n",
    "lgb_model_di = lgb.LGBMClassifier(verbose=-1)\n",
    "xgb_model_di = xgb.XGBClassifier()\n",
    "cat_model_di = CatBoostClassifier(verbose=0)\n",
    "\n",
    "# 전체 데이터를 사용하여 모델 학습 및 예측 함수 정의\n",
    "def train_and_predict(X_train, y_train, X_test, models):\n",
    "    # 모델 학습\n",
    "    for model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    # 예측 확률 생성\n",
    "    pred_probas = [model.predict_proba(X_test)[:, 1] for model in models]\n",
    "    return pred_probas\n",
    "\n",
    "# 공통 특성 찾기\n",
    "common_features_ivf = list(set(IVF_X.columns) & set(IVF_test.columns))\n",
    "common_features_di = list(set(DI_X.columns) & set(DI_test.columns))\n",
    "\n",
    "# ID 열 제거 후 공통 특성을 사용하여 데이터셋 구성\n",
    "IVF_test_common = IVF_test[common_features_ivf].drop(columns=['ID'], errors='ignore')\n",
    "DI_test_common = DI_test[common_features_di].drop(columns=['ID'], errors='ignore')\n",
    "\n",
    "IVF_X_common = IVF_X[common_features_ivf]\n",
    "DI_X_common = DI_X[common_features_di]\n",
    "\n",
    "# IVF 데이터에 대한 예측\n",
    "ivf_models = [lgb_model_ivf, xgb_model_ivf, cat_model_ivf]\n",
    "lgbm_ivf_proba, xgb_ivf_proba, cat_ivf_proba = train_and_predict(IVF_X_common, IVF_y, IVF_test_common, ivf_models)\n",
    "\n",
    "# DI 데이터에 대한 예측\n",
    "di_models = [lgb_model_di, xgb_model_di, cat_model_di]\n",
    "lgbm_di_proba, xgb_di_proba, cat_di_proba = train_and_predict(DI_X_common, DI_y, DI_test_common, di_models)\n",
    "\n",
    "# 메타모델 입력 데이터 생성\n",
    "meta_X_ivf = np.column_stack((lgbm_ivf_proba, xgb_ivf_proba, cat_ivf_proba))\n",
    "meta_X_di = np.column_stack((lgbm_di_proba, xgb_di_proba, cat_di_proba))\n",
    "\n",
    "# 메타모델 훈련\n",
    "meta_model_ivf = LogisticRegression()\n",
    "meta_model_ivf.fit(meta_X_ivf, IVF_y)  # IVF 데이터에 대한 메타모델 학습\n",
    "\n",
    "meta_model_di = LogisticRegression()\n",
    "meta_model_di.fit(meta_X_di, DI_y)  # DI 데이터에 대한 메타모델 학습\n",
    "\n",
    "# 최종 예측\n",
    "final_ivf_predictions = meta_model_ivf.predict(meta_X_ivf)\n",
    "final_di_predictions = meta_model_di.predict(meta_X_di)\n",
    "\n",
    "# 예측 결과 병합\n",
    "IVF_test['probability'] = final_ivf_predictions\n",
    "DI_test['probability'] = final_di_predictions\n",
    "\n",
    "# 최종 제출 파일 생성\n",
    "submission = pd.concat([IVF_test[['ID', 'probability']], DI_test[['ID', 'probability']]], axis=0)\n",
    "submission = submission.sort_values(by='ID')  \n",
    "\n",
    "# 제출 파일 저장\n",
    "submission.to_csv('../submission/code54_stacked.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print(\"제출 파일이 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
